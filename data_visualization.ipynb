{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isaigordeev/Desktop/2023/saxs/saxs\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from saxs.data_generation.processing import Processing\n",
    "from saxs.data_generation.data_visualization import load_data, plot_saxs, plot_saxs_featuremap\n",
    "from saxs.data_generation.generation import Generator\n",
    "from saxs.data_generation import DEFAULT_CONFIG_PATH\n",
    "\n",
    "import json\n",
    "\n",
    "from saxs.gaussian_processing.processing_outils import read_data\n",
    "\n",
    "with open(DEFAULT_CONFIG_PATH) as config:\n",
    "    config_data = json.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q, d1 = load_data(phase=config_data['phase'],\n",
    "                                cubic_mesophase=config_data['cubic_mesophase'],\n",
    "                                )\n",
    "q = q[:498]\n",
    "q_0, I_0, dI = read_data('/Users/isaigordeev/Desktop/2023/saxs/res/075775_treated_xye.csv')\n",
    "I_0 = I_0[:498]\n",
    "q_0 = q_0[:498]\n",
    "mean = np.mean(I_0)\n",
    "var = np.std(I_0)\n",
    "I_0 -= mean\n",
    "I_0 /= (var ** 0.5)\n",
    "I_0 /= np.max(I_0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n in random.sample(range(len(d1)), 10):\n",
    "    # plot_saxs(q , d1[n])\n",
    "    plt.plot(q, d1[n]/np.max(d1[n]))\n",
    "    plt.plot(q_0, I_0/np.max(I_0), 'red')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.uint8(d1[0]*255)\n",
    "plt.imshow(np.repeat(np.expand_dims(np.outer(image, image), -1), 3, axis=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.uint8(I_0/np.max(I_0)*255)\n",
    "plt.imshow(np.repeat(np.expand_dims(np.outer(image, image), -1), 3, axis=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n in random.sample(range(len(d1)), 10):\n",
    "    # plot_saxs(q , d1[n])\n",
    "    plt.plot(q, d1[n]/np.max(d1[n]))\n",
    "    plt.plot(q_0, I_0/np.max(I_0), 'red')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.uint8(d1[0]*255)\n",
    "plt.imshow(np.repeat(np.expand_dims(np.outer(image, image), -1), 3, axis=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(d1[0][68:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "PatchEmbedding(\n",
      "  (patcher): Conv2d(3, 24, kernel_size=(166, 166), stride=(166, 166))\n",
      "  (flatten): Flatten(start_dim=2, end_dim=3)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "========================================================================================================================\nLayer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n========================================================================================================================\nSAXSViT10 (SAXSViT10)                    [1, 3, 498, 498]     [1, 3]               264                  True\n├─PatchEmbedding (patch_embedding)       [1, 3, 498, 498]     [1, 9, 24]           --                   True\n│    └─Conv2d (patcher)                  [1, 3, 498, 498]     [1, 24, 3, 3]        1,984,056            True\n│    └─Flatten (flatten)                 [1, 24, 3, 3]        [1, 24, 9]           --                   --\n├─Dropout (embedding_dropout)            [1, 10, 24]          [1, 10, 24]          --                   --\n├─Sequential (transformer_encoder)       [1, 10, 24]          [1, 10, 24]          --                   True\n│    └─TransformerEncoderLayer (0)       [1, 10, 24]          [1, 10, 24]          153,048              True\n│    └─TransformerEncoderLayer (1)       [1, 10, 24]          [1, 10, 24]          153,048              True\n│    └─TransformerEncoderLayer (2)       [1, 10, 24]          [1, 10, 24]          153,048              True\n├─Sequential (classifier)                [1, 24]              [1, 3]               --                   True\n│    └─LayerNorm (0)                     [1, 24]              [1, 24]              48                   True\n│    └─Linear (1)                        [1, 24]              [1, 3]               75                   True\n========================================================================================================================\nTotal params: 2,443,587\nTrainable params: 2,443,587\nNon-trainable params: 0\nTotal mult-adds (M): 17.86\n========================================================================================================================\nInput size (MB): 2.98\nForward/backward pass size (MB): 0.00\nParams size (MB): 7.94\nEstimated Total Size (MB): 10.91\n========================================================================================================================"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from saxs.saxs_model.model import SAXSViT10\n",
    "\n",
    "from torchinfo import summary #optional\n",
    "\n",
    "from saxs.saxs_model.model_settings import IMAGE_DIM, COLOR_CHANNELS, PATCH_SIZE, ATTENTION_BLOCKS, EMBEDDING_DIM\n",
    "\n",
    "mod = SAXSViT10(IMAGE_DIM,\n",
    "                COLOR_CHANNELS,\n",
    "                PATCH_SIZE,\n",
    "                3,\n",
    "                24,\n",
    "                3072, 12, 0.1, 0, 0.1, 3)\n",
    "\n",
    "print(mod.patch_embedding)\n",
    "\n",
    "\n",
    "\n",
    "summary(mod,\n",
    "        input_size=(1, 3, 498, 498),  # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "30 : LEN OF SAMPLES\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "848a75449aac4ef89a7aeeed2afc89fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'patch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 47\u001B[0m\n\u001B[1;32m     43\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(params\u001B[38;5;241m=\u001B[39mmod\u001B[38;5;241m.\u001B[39mparameters(),\n\u001B[1;32m     44\u001B[0m                              lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[1;32m     45\u001B[0m loss_fn \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[0;32m---> 47\u001B[0m pretrained_vit_results \u001B[38;5;241m=\u001B[39m \u001B[43mengine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     48\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_saxs_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mtest_dataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_saxs_batches\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     50\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m phase_prediction\u001B[38;5;241m.\u001B[39mprediction_from_csv(mod,\n\u001B[1;32m     57\u001B[0m                             \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mres/075773_treated_xye.csv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     58\u001B[0m                             classes,\n\u001B[1;32m     59\u001B[0m                             )\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/tools.py:49\u001B[0m, in \u001B[0;36mxtime_decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     48\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m---> 49\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTaken: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mend_time\u001B[38;5;241m-\u001B[39mstart_time\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m s\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/engine.py:83\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_dataloader, test_dataloader, optimizer, device, loss_fn, epochs)\u001B[0m\n\u001B[1;32m     77\u001B[0m results \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m'\u001B[39m: [],\n\u001B[1;32m     78\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_acc\u001B[39m\u001B[38;5;124m'\u001B[39m: [],\n\u001B[1;32m     79\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_loss\u001B[39m\u001B[38;5;124m'\u001B[39m: [],\n\u001B[1;32m     80\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtest_acc\u001B[39m\u001B[38;5;124m'\u001B[39m: []}\n\u001B[1;32m     82\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(epochs)):\n\u001B[0;32m---> 83\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     84\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43mdataloader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     85\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     86\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     87\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m                                       \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m     test_loss, test_acc \u001B[38;5;241m=\u001B[39m test_step(model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     91\u001B[0m                                     dataloader\u001B[38;5;241m=\u001B[39mtest_dataloader,\n\u001B[1;32m     92\u001B[0m                                     loss_fn\u001B[38;5;241m=\u001B[39mloss_fn,\n\u001B[1;32m     93\u001B[0m                                     device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m     96\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     97\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    100\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    101\u001B[0m     )\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/engine.py:22\u001B[0m, in \u001B[0;36mtrain_step\u001B[0;34m(model, dataloader, loss_fn, optimizer, device)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch, (img, label) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloader):\n\u001B[1;32m     20\u001B[0m     img, label \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mto(device), label\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 22\u001B[0m     label_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(label_pred, label)\n\u001B[1;32m     25\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/Desktop/2023/SAXS/venv38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/model.py:105\u001B[0m, in \u001B[0;36mOriginalViT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    101\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    103\u001B[0m class_token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_embedding\u001B[38;5;241m.\u001B[39mexpand(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 105\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((class_token, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    109\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding \u001B[38;5;241m+\u001B[39m x\n",
      "File \u001B[0;32m~/Desktop/2023/SAXS/venv38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/model.py:47\u001B[0m, in \u001B[0;36mPatchEmbedding.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     46\u001B[0m     image_resolution \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m image_resolution \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput image size must be divisble by patch size, image shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage_resolution\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, patch size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     49\u001B[0m     x_patched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatcher(x)\n\u001B[1;32m     50\u001B[0m     x_flattened \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflatten(x_patched)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'patch_size' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from saxs import PACKAGE_PATH, DEFAULT_PHASES_PATH\n",
    "from saxs.saxs_model.model import SAXSViT10\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import saxs.saxs_model.saxs_dataset as data_setup\n",
    "from saxs import PACKAGE_PATH\n",
    "import json\n",
    "import saxs.saxs_model.phase_prediction as phase_prediction\n",
    "from saxs.saxs_model import engine\n",
    "from saxs.saxs_model.model import SAXSViT\n",
    "from saxs.saxs_model.model_settings import DEVICE\n",
    "\n",
    "from torchinfo import summary #optional\n",
    "\n",
    "from saxs.saxs_model.model_settings import IMAGE_DIM, COLOR_CHANNELS, PATCH_SIZE, ATTENTION_BLOCKS, EMBEDDING_DIM\n",
    "\n",
    "with open(DEFAULT_PHASES_PATH, 'r') as file:  # NOTE make it better with string formatting\n",
    "    phases = json.load(file)\n",
    "\n",
    "classes = list(phases.keys())\n",
    "class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "\n",
    "\n",
    "mod = SAXSViT10(IMAGE_DIM,\n",
    "                COLOR_CHANNELS,\n",
    "                166,\n",
    "                3,\n",
    "                24,\n",
    "                3072, 12, 0.1, 0, 0.1, 3)\n",
    "\n",
    "train_saxs_batches, test_saxs_batches, saxs_phases = \\\n",
    "    data_setup.create_data_batches_from_dataset_files(path=os.path.join(PACKAGE_PATH, 'cache'),\n",
    "                                                    transforms=None,\n",
    "                                                    batch_size=32,\n",
    "                                                    num_workers=0\n",
    "                                                     )\n",
    "\n",
    "# optimizer = torch.optim.Adam(params=mod.parameters(),\n",
    "#                              lr=1e-3)\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#\n",
    "# pretrained_vit_results = engine.train(model=mod,\n",
    "#                                       train_dataloader=train_saxs_batches,\n",
    "#                                       test_dataloader=test_saxs_batches,\n",
    "#                                       optimizer=optimizer,\n",
    "#                                       loss_fn=loss_fn,\n",
    "#                                       epochs=1,\n",
    "#                                       device=DEVICE)\n",
    "\n",
    "\n",
    "phase_prediction.prediction_from_npy(mod,\n",
    "                            'res/075773_treated_xye.csv',\n",
    "                            classes,\n",
    "                            )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
