{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isaigordeev/Desktop/2023/saxs/saxs\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from saxs.data_generation.processing import Processing\n",
    "from saxs.data_generation.data_visualization import load_data, plot_saxs, plot_saxs_featuremap\n",
    "from saxs.data_generation.generation import Generator\n",
    "from saxs.data_generation import DEFAULT_CONFIG_PATH\n",
    "\n",
    "import json\n",
    "\n",
    "from saxs.gaussian_processing.processing_outils import read_data\n",
    "\n",
    "with open(DEFAULT_CONFIG_PATH) as config:\n",
    "    config_data = json.load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q, d1 = load_data(phase=config_data['phase'],\n",
    "                                cubic_mesophase=config_data['cubic_mesophase'],\n",
    "                                )\n",
    "q = q[:498]\n",
    "q_0, I_0, dI = read_data('/Users/isaigordeev/Desktop/2023/saxs/res/075775_treated_xye.csv')\n",
    "I_0 = I_0[:498]\n",
    "q_0 = q_0[:498]\n",
    "mean = np.mean(I_0)\n",
    "var = np.std(I_0)\n",
    "I_0 -= mean\n",
    "I_0 /= (var ** 0.5)\n",
    "I_0 /= np.max(I_0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n in random.sample(range(len(d1)), 10):\n",
    "    # plot_saxs(q , d1[n])\n",
    "    plt.plot(q, d1[n]/np.max(d1[n]))\n",
    "    plt.plot(q_0, I_0/np.max(I_0), 'red')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.uint8(d1[0]*255)\n",
    "plt.imshow(np.repeat(np.expand_dims(np.outer(image, image), -1), 3, axis=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.uint8(I_0/np.max(I_0)*255)\n",
    "plt.imshow(np.repeat(np.expand_dims(np.outer(image, image), -1), 3, axis=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for n in random.sample(range(len(d1)), 10):\n",
    "    # plot_saxs(q , d1[n])\n",
    "    plt.plot(q, d1[n]/np.max(d1[n]))\n",
    "    plt.plot(q_0, I_0/np.max(I_0), 'red')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image = np.uint8(d1[0]*255)\n",
    "plt.imshow(np.repeat(np.expand_dims(np.outer(image, image), -1), 3, axis=-1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(d1[0][68:])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "PatchEmbedding(\n",
      "  (patcher): Conv2d(3, 24, kernel_size=(166, 166), stride=(166, 166))\n",
      "  (flatten): Flatten(start_dim=2, end_dim=3)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": "========================================================================================================================\nLayer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n========================================================================================================================\nSAXSViT10 (SAXSViT10)                    [1, 3, 498, 498]     [1, 3]               264                  True\n├─PatchEmbedding (patch_embedding)       [1, 3, 498, 498]     [1, 9, 24]           --                   True\n│    └─Conv2d (patcher)                  [1, 3, 498, 498]     [1, 24, 3, 3]        1,984,056            True\n│    └─Flatten (flatten)                 [1, 24, 3, 3]        [1, 24, 9]           --                   --\n├─Dropout (embedding_dropout)            [1, 10, 24]          [1, 10, 24]          --                   --\n├─Sequential (transformer_encoder)       [1, 10, 24]          [1, 10, 24]          --                   True\n│    └─TransformerEncoderLayer (0)       [1, 10, 24]          [1, 10, 24]          153,048              True\n│    └─TransformerEncoderLayer (1)       [1, 10, 24]          [1, 10, 24]          153,048              True\n│    └─TransformerEncoderLayer (2)       [1, 10, 24]          [1, 10, 24]          153,048              True\n├─Sequential (classifier)                [1, 24]              [1, 3]               --                   True\n│    └─LayerNorm (0)                     [1, 24]              [1, 24]              48                   True\n│    └─Linear (1)                        [1, 24]              [1, 3]               75                   True\n========================================================================================================================\nTotal params: 2,443,587\nTrainable params: 2,443,587\nNon-trainable params: 0\nTotal mult-adds (M): 17.86\n========================================================================================================================\nInput size (MB): 2.98\nForward/backward pass size (MB): 0.00\nParams size (MB): 7.94\nEstimated Total Size (MB): 10.91\n========================================================================================================================"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from saxs.saxs_model.model import SAXSViT10\n",
    "\n",
    "from torchinfo import summary #optional\n",
    "\n",
    "from saxs.saxs_model.model_settings import IMAGE_DIM, COLOR_CHANNELS, PATCH_SIZE, ATTENTION_BLOCKS, EMBEDDING_DIM\n",
    "\n",
    "mod = SAXSViT10(IMAGE_DIM,\n",
    "                COLOR_CHANNELS,\n",
    "                PATCH_SIZE,\n",
    "                3,\n",
    "                24,\n",
    "                3072, 12, 0.1, 0, 0.1, 3)\n",
    "\n",
    "print(mod.patch_embedding)\n",
    "\n",
    "\n",
    "\n",
    "summary(mod,\n",
    "        input_size=(1, 3, 498, 498),  # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isaigordeev/Desktop/2023/saxs/saxs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaigordeev/Desktop/2023/SAXS/venv38/lib/python3.8/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/isaigordeev/Desktop/2023/SAXS/venv38/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/Users/isaigordeev/Desktop/2023/SAXS/venv38/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "SAMPLE IS GREAT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'patch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 43\u001B[0m\n\u001B[1;32m     29\u001B[0m mod \u001B[38;5;241m=\u001B[39m SAXSViT10(IMAGE_DIM,\n\u001B[1;32m     30\u001B[0m                 COLOR_CHANNELS,\n\u001B[1;32m     31\u001B[0m                 \u001B[38;5;241m166\u001B[39m,\n\u001B[1;32m     32\u001B[0m                 \u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m     33\u001B[0m                 \u001B[38;5;241m24\u001B[39m,\n\u001B[1;32m     34\u001B[0m                 \u001B[38;5;241m3072\u001B[39m, \u001B[38;5;241m12\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0.1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# train_saxs_batches, test_saxs_batches, saxs_phases = \\\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m#     data_setup.create_data_batches_from_dataset_files(path=os.path.join(PACKAGE_PATH, 'cache'),\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m#                                                     transforms=None,\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m#                                                     batch_size=32,\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m#                                                     num_workers=0\u001B[39;00m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m#                                                      )\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m \u001B[43mphase_prediction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprediction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m                            \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mres/075773_treated_xye.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mclasses\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[43m                            \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/phase_prediction.py:34\u001B[0m, in \u001B[0;36mprediction\u001B[0;34m(model, path_csv, class_names, transforms, cut_start, image_size, device)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39minference_mode():\n\u001B[1;32m     33\u001B[0m     transformed_phase_img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39munsqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 34\u001B[0m     predicted_phase_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformed_phase_img\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m     predicted_phase_probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(predicted_phase_tensor, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     38\u001B[0m     predicted_phase_label \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(predicted_phase_probs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/2023/SAXS/venv38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/model.py:105\u001B[0m, in \u001B[0;36mOriginalViT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    101\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    103\u001B[0m class_token \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclass_embedding\u001B[38;5;241m.\u001B[39mexpand(batch_size, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 105\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((class_token, x), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    109\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mposition_embedding \u001B[38;5;241m+\u001B[39m x\n",
      "File \u001B[0;32m~/Desktop/2023/SAXS/venv38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/2023/saxs/saxs/saxs_model/model.py:47\u001B[0m, in \u001B[0;36mPatchEmbedding.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     46\u001B[0m     image_resolution \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m image_resolution \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_size \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput image size must be divisble by patch size, image shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimage_resolution\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, patch size: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     49\u001B[0m     x_patched \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatcher(x)\n\u001B[1;32m     50\u001B[0m     x_flattened \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mflatten(x_patched)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'patch_size' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from saxs import PACKAGE_PATH, DEFAULT_PHASES_PATH\n",
    "from saxs.saxs_model.model import SAXSViT10\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import saxs.saxs_model.saxs_dataset as data_setup\n",
    "from saxs import PACKAGE_PATH\n",
    "import json\n",
    "import saxs.saxs_model.phase_prediction as phase_prediction\n",
    "from saxs.saxs_model import engine\n",
    "from saxs.saxs_model.model import SAXSViT\n",
    "from saxs.saxs_model.model_settings import DEVICE\n",
    "\n",
    "from torchinfo import summary #optional\n",
    "\n",
    "from saxs.saxs_model.model_settings import IMAGE_DIM, COLOR_CHANNELS, PATCH_SIZE, ATTENTION_BLOCKS, EMBEDDING_DIM\n",
    "\n",
    "with open(DEFAULT_PHASES_PATH, 'r') as file:  # NOTE make it better with string formatting\n",
    "    phases = json.load(file)\n",
    "\n",
    "classes = list(phases.keys())\n",
    "class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "\n",
    "\n",
    "mod = SAXSViT10(IMAGE_DIM,\n",
    "                COLOR_CHANNELS,\n",
    "                166,\n",
    "                3,\n",
    "                24,\n",
    "                3072, 12, 0.1, 0, 0.1, 3)\n",
    "\n",
    "# train_saxs_batches, test_saxs_batches, saxs_phases = \\\n",
    "#     data_setup.create_data_batches_from_dataset_files(path=os.path.join(PACKAGE_PATH, 'cache'),\n",
    "#                                                     transforms=None,\n",
    "#                                                     batch_size=32,\n",
    "#                                                     num_workers=0\n",
    "#                                                      )\n",
    "\n",
    "phase_prediction.prediction_from_csv(mod,\n",
    "                            'res/075773_treated_xye.csv',\n",
    "                            classes,\n",
    "                            )\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
